# -*- coding: utf-8 -*-
"""Assignment_Extra_Class9_Feature_Importance_Analysis & Model_Interpretation_DimasAdiPrasetyo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zK7zdeqMq_55L8CZlXB6OJVbAcDEkHsE
"""

from google.colab import drive

import numpy as np

import pandas as pd

from scipy.stats import chi2_contingency, randint, uniform, ttest_ind
from scipy.stats.contingency import association

import seaborn as sns

import shap

from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
  classification_report,
  PrecisionRecallDisplay,
  RocCurveDisplay,
)
from sklearn.model_selection import (
  cross_val_score,
  train_test_split,
  RandomizedSearchCV,
)
from sklearn.preprocessing import (
  LabelEncoder,
  MinMaxScaler,
  OneHotEncoder,
  OrdinalEncoder,
)

import xgboost as xgb

HEX_BLUE = '#48a8c4'
HEX_RED = '#c0504d'

"""# Prepare data"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Dibimbing Bootcamp Data Science/bank_churn_data.csv')
df.shape

df.sample(3)

"""## Check data types and missing values"""

df.info()

df.describe()

df.isnull().sum()

df['attrition_flag'].value_counts(normalize=True)

# Buat label target
df['churn_flag'] = df['attrition_flag'].apply(lambda x: 1 if x == 'Attrited Customer' else 0)

# Mapping 1 -> Yes, 0 -> No
df['churn_flag'] = df['churn_flag'].map({1: 'Yes', 0: 'No'})

# Drop kolom yang tidak berguna
df.drop(['attrition_flag'], axis=1, inplace=True)

df['churn_flag'].value_counts(normalize=True)

# cek outlier dari data

# Function to detect outliers using IQR
def detect_outliers_iqr(df, column):
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Example usage for numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

import matplotlib.pyplot as plt
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Box plot of {col}')
    plt.show()

"""# EDA"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='customer_age', hue='churn_flag', multiple='stack', bins=20)
plt.title('Age Distribution by Churn Status')
plt.xlabel('Customer Age')
plt.ylabel('Count')
plt.show()

# Churn rate by age group
df['age_group'] = pd.cut(df['customer_age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])
churn_by_age = df.groupby('age_group')['churn_flag'].value_counts(normalize=True).unstack().fillna(0)
churn_by_age['Churn_Rate'] = churn_by_age['Yes']
print("\nChurn Rate by Age Group:")
print(churn_by_age[['Churn_Rate']])

"""**Insight dari distribusi Usia dan Churn**
- Distribusi usia pelanggan terpusat antara 40 hingga 60 tahun, dengan paling banyak di usia sekitar 45–50 tahun.

- Churn Rate tertinggi berada di kelompok usia 50–60 tahun (16.76%) dan 40–50 tahun (16.74%).

- Churn paling rendah di kelompok 20–30 tahun (12.08%).

Pelanggan berusia 40–60 tahun memiliki kemungkinan churn yang lebih tinggi dibanding kelompok usia lainnya. Segmentasi dan strategi retensi harus difokuskan pada rentang usia ini.
"""

df['income_category'].value_counts(normalize=True)

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['education_level'], df['income_category'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('Education Level vs. Income Category (Normalized)')
plt.show()

churn_by_edu = df.groupby('education_level')['churn_flag'].value_counts(normalize=True).unstack().fillna(0)
churn_by_income = df.groupby('income_category')['churn_flag'].value_counts(normalize=True).unstack().fillna(0)
print("\nChurn Rate by Education Level:")
print(churn_by_edu[['Yes']])
print("\nChurn Rate by Income Category:")
print(churn_by_income[['Yes']])

"""**Insight dari Income Category vs Churn**

- Pelanggan dengan pendapatan **tertinggi** (`$120K+`) dan **terendah** (`< $40K`) sama-sama memiliki **churn rate yang tinggi**, yaitu sekitar **17.1%–17.3%**.
- Sementara itu, pelanggan dengan pendapatan **menengah** (`$60K – $80K`) justru memiliki **churn rate terendah**, yaitu sebesar **13.48%**.

Pelanggan dengan pendapatan menengah cenderung lebih loyal, sementara yang sangat kaya atau sangat miskin lebih berisiko churn.

**Insight dari Tingkat Pendidikan vs Churn**

- Heatmap menunjukkan hampir semua tingkat pendidikan memiliki proporsi pendapatan terbesar di kategori Less than $40K.

- Pendidikan Doctorate (21.06%) dan Post-Graduate (17.83%) memiliki churn tertinggi.

- Pendidikan High School (15.2%) dan Graduate (15.5%) memiliki churn lebih rendah.

Walaupun pendidikan tinggi umumnya diasosiasikan dengan pendapatan lebih besar, pelanggan dengan pendidikan lebih tinggi justru cenderung churn lebih besar. Mungkin karena ekspektasi layanan yang lebih tinggi atau lebih mudah pindah ke penyedia lain.
"""

# Churn by Gender
churn_by_gender = df.groupby('gender')['churn_flag'].value_counts(normalize=True).unstack().fillna(0)
print("\nChurn Rate by Gender:")
print(churn_by_gender[['Yes']])

"""**Insight Gender vs Churn**

- Perempuan (17.36%) lebih banyak churn dibanding laki-laki (14.62%).

Perlu analisis lebih lanjut mengapa perempuan lebih berisiko churn. Mungkin terkait preferensi layanan, komunikasi, atau penawaran produk.
"""

# Months on Book vs. Churn
plt.figure(figsize=(10, 6))
sns.boxplot(x='churn_flag', y='months_on_book', data=df)
plt.title('Months on Book by Churn Status')
plt.show()

"""**Insight dari Months on Book vs Churn**

- Boxplot menunjukkan tidak ada perbedaan signifikan dalam lama menjadi nasabah antara churn dan tidak churn.

Durasi menjadi nasabah tidak berkorelasi langsung dengan churn. Pelanggan lama tidak selalu lebih loyal — perlu pendekatan retensi lebih personal.
"""

# Contacts in Last 12 Months vs. Churn
plt.figure(figsize=(10, 6))
sns.boxplot(x='churn_flag', y='contacts_count_12_mon', data=df)
plt.title('Contacts in Last 12 Months by Churn Status')
plt.show()

"""**Insight dari Kontak Bank vs Churn**

- Pelanggan yang churn memiliki jumlah kontak dengan bank yang lebih tinggi.

Pelanggan yang sering menghubungi bank justru cenderung churn. Ini bisa menjadi tanda ketidakpuasan atau masalah yang belum terselesaikan.
"""

plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""## Data distribution"""

target_column = 'churn_flag'

ordinal_columns = [
    'education_level',
    'income_category',
    'card_category'
]

nominal_columns = [
    'gender',
    'marital_status'
]

category_columns = ordinal_columns + nominal_columns

numeric_columns = [
    'customer_age',
    'dependent_count',
    'months_on_book',
    'total_relationship_count',
    'months_inactive_12_mon',
    'contacts_count_12_mon',
    'credit_limit',
    'total_revolving_bal',
    'avg_open_to_buy',
    'total_amt_chng_q4_q1',
    'total_trans_amt',
    'total_trans_ct',
    'total_ct_chng_q4_q1',
    'avg_utilization_ratio'
]

df.groupby(target_column).size()

"""### Category"""

df_dist_cat = pd.DataFrame()

for column in category_columns:
    df_temp = df.pivot_table(
        index=column,
        columns='churn_flag',
        values='user_id',
        aggfunc='count'
    )
    res = chi2_contingency(df_temp)
    coef = round(association(df_temp, method='cramer'), 2) # cramer's V

    index_tuples = [(column, x) for x in df_temp.index]
    df_temp.index = pd.MultiIndex.from_tuples(
        index_tuples,
        names=['column', 'value'],
    )

    df_temp['churn_proba'] = df_temp['Yes'] / (df_temp['No'] + df_temp['Yes'])
    df_temp['pvalue'] = ('' if res.pvalue > 0.05 else '< 0.05')
    df_temp['coef'] = coef

    df_dist_cat = pd.concat([df_dist_cat, df_temp])

df_dist_cat

"""### Numeric"""

def show_distribution_numeric(column):
    res = ttest_ind(
        df[df['churn_flag'] == 'No'][column],
        df[df['churn_flag'] == 'Yes'][column],
        equal_var=False,
        alternative='two-sided',
    )
    print(f'p-value: {res.pvalue:.2f}')

    # Plot
    plt.figure(figsize=(6, 5))
    sns.violinplot(
        data=df,
        x=column,
        y='churn_flag',
        hue='churn_flag',
        split=True,
        inner='quart',
        palette={'Yes': HEX_RED, 'No': HEX_BLUE},
        orient='h'
    )
    plt.title(f'Distribution of {column} by Churn')
    plt.xlabel(column)
    plt.ylabel('churn_flag')
    plt.legend(title='churn_flag', loc='lower right')
    plt.tight_layout()
    plt.show()

show_distribution_numeric('months_inactive_12_mon')

show_distribution_numeric('customer_age')

show_distribution_numeric('total_trans_amt')

show_distribution_numeric('avg_utilization_ratio')

"""## Bivariate

### Category & category
"""

def show_relationship_category_category(column1, column2):
    df_temp = df.pivot_table(
        index=column1,
        columns=column2,
        values='user_id',
        aggfunc='count',
    )
    df_temp = df_temp.fillna(0).astype(int)

    res = chi2_contingency(df_temp)
    coef = round(association(df_temp, method='cramer'), 2)

    return df_temp, ('' if res.pvalue > 0.05 else '< 0.05'), coef

df_corr_cat_cat = pd.DataFrame()

for i in range(len(category_columns)):
    column1 = category_columns[i]
    for j in range(i + 1, len(category_columns)):
        column2 = category_columns[j]
        _, pvalue, coef = show_relationship_category_category(column1, column2)

        df_temp = pd.DataFrame({
            'column1': [column1],
            'column2': [column2],
            'pvalue': [pvalue],
            'coef': [coef],
        })
        df_corr_cat_cat = pd.concat([df_corr_cat_cat, df_temp], ignore_index=True)

df_corr_cat_cat

df_temp, pvalue, coef = show_relationship_category_category(
    'gender',
    'marital_status',
)
print('p-value:', pvalue)
print("Cramer's V:", coef)

sns.heatmap(df_temp, cmap='hot_r', annot=True, fmt='.0f')
plt.title('Heatmap: gender vs marital_status')
plt.show()

df_temp = df_corr_cat_cat.pivot_table(
  index='column1',
  columns='column2',
  values='coef',
)

for column1 in df_corr_cat_cat['column1']:
  for column2 in df_corr_cat_cat['column2']:
    if np.isnan(df_temp.loc[column1, column2]) and column1 < column2:
      df_temp.loc[column1, column2] = df_temp.loc[column2, column1]
      df_temp.loc[column2, column1] = None

sns.heatmap(
  df_temp,
  vmin = 0,
  vmax = 1,
  cmap = 'hot_r',
)

"""INSIGHT

- No strong multicollinearity between categorical features.

- All columns are retained for modeling due to business relevance.

### Numeric & numeric
"""

df[numeric_columns].corr()

sns.pairplot(df[numeric_columns])

"""### Category & numeric"""

def show_relationship_category_numeric(cat, num):
        df_temp = df[[cat, num, 'user_id']].dropna().copy()
        qcut_result, bins = pd.qcut(
            df_temp[num],
            q=4,
            retbins=True,
            duplicates='drop'
        )
        num_bins = len(bins) - 1
        if num_bins < 2:
            raise ValueError("Too few bins to perform meaningful comparison.")

        labels = ['low', 'midlow', 'midhigh', 'high'][:num_bins]

        df_temp[num + '_bin'] = pd.qcut(
            df_temp[num],
            q=num_bins,
            labels=labels,
            duplicates='drop'
        )
        df_pivot = df_temp.pivot_table(
            index=cat,
            columns=num + '_bin',
            values='user_id',
            aggfunc='nunique',
            observed=False
        ).fillna(0).astype(int)

        # Chi-square & Cramer's V
        res = chi2_contingency(df_pivot)
        coef = round(association(df_pivot, method='cramer'), 2)

        return df_pivot, ('< 0.05' if res.pvalue < 0.05 else ''), coef

df_corr_cat_num = pd.DataFrame()

for i in range(len(category_columns)):
  column1 = category_columns[i]
  for j in range(len(numeric_columns)):
    column2 = numeric_columns[j]
    _, pvalue, coef = show_relationship_category_numeric(column1, column2)

    df_temp = pd.DataFrame({
      'column1': [column1],
      'column2': [column2],
      'pvalue': [pvalue],
      'coef': [coef],
    })
    df_corr_cat_num = pd.concat([df_corr_cat_num, df_temp])

df_corr_cat_num

sns.violinplot(
  df,
  x='months_on_book',
  y='churn_flag',
  split = True,
  inner = 'quart',
  palette = {'No': HEX_BLUE, 'Yes': HEX_RED},
  orient = 'h'
)

"""# Preprocess dataset

## Split dataset
"""

target_column = 'churn_flag'

X_train, X_test, y_train, y_test = train_test_split(
    df.drop(columns=target_column),
    df[target_column],
    test_size=0.2,
    random_state=11,
    stratify=df[target_column]
)

X_train.shape

X_test.shape

"""## Data transformation

### Drop columns
"""

dropped_columns = ['user_id']

X_train = X_train.drop(columns=dropped_columns)
X_train.shape

X_test = X_test.drop(columns=dropped_columns)
X_test.shape

"""### Encode target variable"""

encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)
['{0}: {1}'.format(i, v) for i, v in enumerate(encoder.classes_)]

"""### Encode ordinal variables"""

edu_order = [
    'Unknown', 'Uneducated', 'High School', 'College',
    'Graduate', 'Post-Graduate', 'Doctorate'
]
encoder_edu = OrdinalEncoder(categories=[edu_order])
X_train['education_level'] = encoder_edu.fit_transform(X_train[['education_level']])
X_test['education_level'] = encoder_edu.transform(X_test[['education_level']])

income_order = [
    'Unknown', 'Less than $40K', '$40K - $60K',
    '$60K - $80K', '$80K - $120K', '$120K +'
]
encoder_income = OrdinalEncoder(categories=[income_order])
X_train['income_category'] = encoder_income.fit_transform(X_train[['income_category']])
X_test['income_category'] = encoder_income.transform(X_test[['income_category']])

card_order = ['Blue', 'Silver', 'Gold', 'Platinum']
encoder_card = OrdinalEncoder(categories=[card_order])
X_train['card_category'] = encoder_card.fit_transform(X_train[['card_category']])
X_test['card_category'] = encoder_card.transform(X_test[['card_category']])

binary_map_gender = {'F': 0, 'M': 1}
X_train['gender'] = X_train['gender'].map(binary_map_gender)
X_test['gender'] = X_test['gender'].map(binary_map_gender)

binary_map_marital = {'Single': 0, 'Married': 1, 'Divorced': 2}
X_train['marital_status'] = X_train['marital_status'].map(binary_map_marital)
X_test['marital_status'] = X_test['marital_status'].map(binary_map_marital)

"""### Encode nominal variables"""

def custom_feature_name(transformer_name, feature_name):
    return f'{feature_name}'

encoder = OneHotEncoder(drop='first', handle_unknown='ignore')


transformer = ColumnTransformer(
    transformers=[
        ('onehotencoder', encoder, nominal_columns),
    ],
    verbose_feature_names_out=False
)

transformed_train = transformer.fit_transform(X_train)

df_encoded_train = pd.DataFrame(
    transformed_train.toarray(),
    columns=transformer.get_feature_names_out(),
    index=X_train.index
)

X_train = pd.concat([X_train.drop(columns=nominal_columns), df_encoded_train], axis=1)
X_train.sample(5)

transformed_test = transformer.transform(X_test)

df_encoded_test = pd.DataFrame(
    transformed_test.toarray(),
    columns=transformer.get_feature_names_out(),
    index=X_test.index
)

X_test = pd.concat([X_test.drop(columns=nominal_columns), df_encoded_test], axis=1)
X_test.sample(5)

"""### Scale numeric variables"""

scaler = MinMaxScaler()

X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])

X_train[numeric_columns].sample(5)

# Drop the 'age_group' column which was used for EDA and contains string values
X_train = X_train.drop(columns=['age_group'])
X_test = X_test.drop(columns=['age_group'])

"""# Select model

## Logistic regression

### Training model
"""

model = LogisticRegression(
  penalty = 'l2',
  C = 1.0,
  class_weight = 'balanced',
  random_state = 11,
  solver = 'lbfgs',
)

model.fit(X_train, y_train)

scores = cross_val_score(
  model,
  X_train,
  y_train,
  cv = 5,
  scoring = 'f1',
)

print('Mean :', round(scores.mean(), 3))
print('STD :', round(scores.std(), 3))

sns.boxplot(x=scores, orient='h')
sns.stripplot(x=scores, color='red')

"""### Evaluation"""

y_train_pred = model.predict(X_train)
print(classification_report(y_train, y_train_pred))

y_test_pred = model.predict(X_test)
print(classification_report(y_test, y_test_pred))

auc_train = RocCurveDisplay.from_estimator(
  model,
  X_train,
  y_train,
  name='Train',
)

RocCurveDisplay.from_estimator(
  model,
  X_test,
  y_test,
  name='Test',
  ax=auc_train.ax_,
)

auc_train = PrecisionRecallDisplay.from_estimator(
  model,
  X_train,
  y_train,
  name='Train',
)

PrecisionRecallDisplay.from_estimator(
  model,
  X_test,
  y_test,
  name='Test',
  ax=auc_train.ax_,
)

"""### Interpretation"""

df_feature = pd.DataFrame({
  'feature': X_train.columns,
  'coef': model.coef_[0],
  'coef_abs': abs(model.coef_[0]),
})

df_feature.sort_values('coef_abs', ascending=False)

"""## Random Forest"""

space = {
  'learning_rate': uniform(loc=0.3, scale=0.7),
  'gamma': uniform(loc=1, scale=8),
  'max_depth': randint(3, 12),
  'reg_lambda' : uniform(loc=0, scale=1),
  'reg_alpha' : randint(0, 180),
  'colsample_bytree' : uniform(loc=0.5, scale=0.5),
  'min_child_weight' : randint(0, 10),
  'n_estimators': [100, 140, 180],
}

model = xgb.XGBClassifier(
  scale_pos_weight = (y_train.shape[0] - sum(y_train)) / sum(y_train),
)

hp = RandomizedSearchCV(
  model,
  space,
  n_iter = 10,
  scoring = 'f1',
  return_train_score = True,
  random_state = 11,
)

search = hp.fit(X_train, y_train)

pd.DataFrame.from_dict(search.cv_results_).sort_values('rank_test_score')

search.best_score_

model = search.best_estimator_
model.fit(X_train, y_train)
scores = cross_val_score(
  model,
  X_train,
  y_train,
  cv = 5,
  scoring = 'f1',
)

print('Mean :', round(scores.mean(), 3))
print('STD :', round(scores.std(), 3))

sns.boxplot(x=scores, orient='h')
sns.stripplot(x=scores, color='red')

"""### Evaluation"""

y_train_pred = model.predict(X_train)
print(classification_report(y_train, y_train_pred))

y_test_pred = model.predict(X_test)
print(classification_report(y_test, y_test_pred))

# ROC Curve
auc_train = RocCurveDisplay.from_estimator(
    model,
    X_train,
    y_train,
    name='Train',
)

RocCurveDisplay.from_estimator(
    model,
    X_test,
    y_test,
    name='Test',
    ax=auc_train.ax_,
)

# Precision-Recall Curve
auc_train = PrecisionRecallDisplay.from_estimator(
    model,
    X_train,
    y_train,
    name='Train',
)

PrecisionRecallDisplay.from_estimator(
    model,
    X_test,
    y_test,
    name='Test',
    ax=auc_train.ax_,
)

"""### Interpretation"""

xgb.plot_importance(model, importance_type='weight')

xgb.plot_importance(model, importance_type='gain')

"""# Explain model with SHAP"""

explainer = shap.TreeExplainer(
  model,
  X_train,
  feature_perturbation='interventional',
  model_output='probability',
)
explanation = explainer(X_train)
shap_values = explainer.shap_values(X_train)

shap.Explainer(model, X_train)

"""## Feature importance"""

shap.summary_plot(shap_values, X_train, plot_type='bar')

"""## Beeswarm plot"""

shap.summary_plot(shap_values, X_train)

"""## Partial dependence plot"""

shap.dependence_plot(
  'total_trans_ct',
  shap_values,
  X_train,
)

"""## Waterfall"""

shap.plots.waterfall(explanation[11])

"""# Insight dan Rekomendasi Bisnis

## Insight dari Analisis & Model
- Pelanggan dengan **aktivitas transaksi rendah**, **penurunan frekuensi transaksi antar kuartal**, serta **penggunaan kredit rendah (low utilization ratio)** memiliki probabilitas churn yang tinggi.
- Fitur seperti `total_trans_ct`, `total_ct_chng_q4_q1`, dan `avg_utilization_ratio` menjadi penentu utama dalam prediksi churn berdasarkan hasil SHAP.
- **Usia tidak memiliki hubungan signifikan**, namun kelompok usia 50+ menunjukkan kecenderungan churn sedikit lebih tinggi.
- **Durasi menjadi nasabah (months_on_book)** tidak menjamin loyalitas — pelanggan lama pun tetap bisa churn.

## Rekomendasi
1. **Segmentasi Nasabah Berisiko Tinggi**  
   Fokuskan retensi pada pelanggan dengan:
   - Penurunan aktivitas transaksi (`total_ct_chng_q4_q1` < 0.5)
   - Transaksi tahunan rendah (`total_trans_ct` < 50)
   - Utilisasi rendah (`avg_utilization_ratio` < 0.1)

2. **Program Loyalitas dan Aktivasi**  
   Berikan insentif atau promosi khusus untuk:
   - Meningkatkan frekuensi penggunaan kartu
   - Mendorong repeat transaction dari nasabah dengan pola transaksi menurun

3. **Monitoring & Notifikasi Dini**  
   Buat sistem alert untuk mengenali nasabah dengan tanda-tanda potensi churn berdasarkan fitur penting (misal penurunan transaksi signifikan dalam 3 bulan terakhir).

4. **Evaluasi Interaksi Layanan**  
   Pelanggan dengan interaksi minim terhadap bank (`contacts_count_12_mon`) juga bisa menjadi target pendekatan personalisasi.
"""

